
# ğŸ® Reinforcement Learning: REINFORCE for CartPole ğŸš€

Welcome to the **REINFORCE CartPole Project**! This repository showcases a simple reinforcement learning implementation using the REINFORCE algorithm to solve the classic **CartPole** environment. ğŸ—ï¸

---

## ğŸŒŸ Features
- **Environment**: CartPole-v1 from OpenAI Gym ğŸ¤–
- **Algorithm**: Policy Gradient-based REINFORCE ğŸ§ 
- **Library**: Implemented using JAX and Haiku for speed and flexibility âš¡
- **Goal**: Balance a pole on a cart for as long as possible. The environment is considered solved with an average score of **500**! ğŸ†

---

## ğŸ“‚ File Structure
- `cartpole.ipynb` ğŸ“’: The core implementation with training, evaluation, and visualization.
- ğŸ“Š Includes reward plotting for training performance monitoring.

---

## ğŸ“¦ Dependencies
Ensure you have the following installed:
- JAX ğŸ¦
- Gym ğŸ‹ï¸â€â™‚ï¸
- Haiku ğŸŒ²
- Optax âš™ï¸
- Matplotlib ğŸ“ˆ

Install them with:
```bash
pip install jaxlib jax gym[box2d] git+https://github.com/deepmind/dm-haiku optax matplotlib
```

---

## ğŸš€ Getting Started
1. Clone this repository:  
   ```bash
   git clone https://github.com/Mamoro98/REINFORCE-cartpole.git
   ```
2. Navigate to the directory:  
   ```bash
   cd REINFORCE-cartpole
   ```
3. Run the notebook:  
   ```bash
   jupyter notebook cartpole.ipynb
   ```

---

## ğŸ§  How It Works
1. **State Space**: ğŸ–¥ï¸ The agent observes:
   - Pole angle ğŸ“
   - Angular velocity â±ï¸
   - Cart position â¡ï¸
   - Cart velocity â¬…ï¸
2. **Action Space**: ğŸ¤” Two possible actions:
   - Move cart **left** â¬…ï¸
   - Move cart **right** â¡ï¸
3. **Reward**: ğŸ¯
   - +1 for each timestep the pole remains upright.

The REINFORCE algorithm optimizes a neural network policy through the **Policy Gradient Theorem**.

---

## ğŸ› ï¸ Training Tips
- Experiment with hyperparameters like learning rate or neural network architecture. ğŸ”§
- Monitor reward trends to detect overfitting or undertraining. ğŸ“ˆ

---

## ğŸŒ Credits
Developed during the **Reinforcement Learning (from Zero to One)** course at AIMS South Africa. Adapted from the Deep Learning Indaba series. ğŸ“š

---

## ğŸ‰ Results
The trained agent achieves an average score of **500**, solving the environment successfully! ğŸ’¯
Video: https://github.com/user-attachments/assets/8f7e12d9-c828-4f71-8151-d4c6e62dc567


Feel free to fork, star â­, or contribute to this project! 

--- 

